# ğŸš— Vehicle Sales Data

## ğŸŒŸ Overview

The **Vehicle Sales Data** is designed to process and analyze data related to vehicle sales from multiple sources, such as CSV files, APIs, and SQL databases. The project uses **Azure Data Factory (ADF)** for data ingestion and transformation, **Databricks** for advanced processing and cataloging, and **SQL Warehouse** for querying and analytics. The **Medalion Architecture** is implemented in this project.

The data is organized into three layers:
- **Bronze**: Raw data stored in Parquet format.
- **Silver**: Transformed and cleansed data in Parquet format.
- **Gold**: Aggregated data ready for analytics, stored as facts and dimensions in Native(Delta) format.

## ğŸ› ï¸ Workflow

1. **ğŸ“¥ Data Ingestion**:
   - Data is extracted from GitHub (CSV files), APIs, and SQL databases into SQL DB using Azure Data Factory pipelines.

2. **ğŸ”„ Incremental Loading**:
   - High Water Mark logic is implemented to load only new or updated records into the Bronze layer in ADLS.

3. **ğŸ“‚ Data Transformation**:
   - Databricks is used to create a catalog and schemas for  Silver and Gold layers.
   - Data transformations are applied, and the results are stored in ADLS as well.

4. **ğŸ“Š Data Aggregation**:
   - Facts and dimensions are created for the Gold layer, enabling advanced analytics.

5. **ğŸ”’ Credential Management**:
   - Secrets for accessing various services are stored securely in Azure Key Vault and accessed via app registrations.

6. **ğŸ“ˆ Analytics**:
   - SQL Warehouse is used to run aggregation queries and generate insights.

## ğŸ“‚ Repository Structure
```plain.text
Vehicle-Sales-Data-Project/
â”œâ”€â”€ databricks/             # Databricks notebooks for transformations
â”œâ”€â”€ dataset/                # Datasets in ADF
â”œâ”€â”€ datasets/               # Sample data used for testing
â”œâ”€â”€ factory/                # ARM templates and Global Parameters
â”œâ”€â”€ linkedservice/          # Linked services in ADF
â”œâ”€â”€ pipeline/               # ADF pipelines configurations
â”œâ”€â”€ architecture/           # Architecture diagram
â”œâ”€â”€ README.md               # Project overview (this file)
```
## ğŸš€ Key Features

- ğŸ“¤ **Data Extraction**: Automated data ingestion from multiple sources.
- ğŸ§¹ **Data Cleansing**: Transform raw data into structured formats.
- ğŸ“œ **Cataloging**: Organized data into Silver and Gold schemas.
- ğŸ” **Incremental Loading**: Efficiently loads new/updated records using High Water Mark logic.
- ğŸ”’ **Secure Credentials**: Managed securely via Azure Key Vault.
- ğŸ“Š **Analytics Ready**: Fact and dimension tables for data aggregation and analysis.

## ğŸ› ï¸ Technologies Used
- ğŸ—„ï¸ **Azure Data Lake Storage (ADLS)**: Centralized storage for raw and processed data.
- ğŸ”„ **Azure Data Factory (ADF)**: Orchestration tool for data pipelines.
- ğŸ” **Azure Key Vault**: Securely manages access credentials for services.
- ğŸ§ª **Azure Databricks**: Handles data transformations and cataloging.
- ğŸ“œ **SQL Warehouse**: Enables querying and advanced analytics.

## ğŸš€ How to Use
1. **ğŸ”‘ Setup Credentials**:
   - Store necessary credentials in Azure Key Vault and grant access to app registrations.
2. **ğŸ”„ Run Pipelines**:
   - Use ADF pipelines to ingest data into the Bronze layer.
3. **ğŸ“‚ Process Data**:
   - Execute Databricks notebooks to process data into Silver and Gold layers.
4. **ğŸ“Š Analyze Data**:
   - Use SQL Warehouse to run aggregation queries and generate insights.

## ğŸŒŸ Future Enhancements
- ğŸ§  Add ML models for predictive sales analytics.
- ğŸ“Š Create real-time dashboards for sales insights.
- âš¡ Optimize pipeline performance with partitioning and caching.

## ğŸ“¬ Contact
For any questions or support, reach out to the repository maintainer at **[vignanpatchigolla5@gmail.com]**.
